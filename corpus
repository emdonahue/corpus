#!/bin/zsh

#TODO Make mwe match multiline expressions.
#TODO Give merge option to split lines to make large token files readable by other line-based tools. Possibly using null bytes to preserve original linebreaks.

set -o errexit
set -o pipefail
if [[ "${TRACE-0}" == "1" ]]; then set -o xtrace; fi

unset c #Reserved variable for inline comments

FS=$'\034'
GS=$'\035'
US=$'\037'

CMD="$1"
[[ $# -ne 0 ]] && shift
case "$CMD" in
    --help|-h|help) #[SUBCOMMAND]; Prints help text for SUBCOMMAND. If SUBCOMMAND omitted, prints list of subcommands.
	[[ $# -eq 0 ]] && echo 'corpus - A suite of tools for cleaning textual corpora. \nUsage: corpus SUBCOMMAND [ARGUMENTS...]\n\nSubcommands:'
	sed -nE '/\s*'"$1"'\)\s#/'"${1:+,/^\s*;;\s*$/}"'{s/^[[:space:]]*([-|[:alnum:]]+)\)\s#([^;]*); (.*)/'"${${1-\t}:#$1}"'\1 \2\t\3/p; s/.*(\w+)[-+:]*=\w+ \$\{c#(.*); (.*)\}.*/\t-\1 \2\t\3/p}' "$0"
	;;
    
    charset) #[FILE...]; Prints counts of every character used in the corpus, substituting ASCII octal codes for control characters. Also counts filename metadata if corpus merged.
	awk -l ordchr -vRS='(.)' '{c[RT]++} END {asorti(c,ab); for (i in ab) {if (0 <= ord(ab[i]) && ord(ab[i]) <=32) {printf("$'"'"'\\\\%03d'"'"'",ord(ab[i]))} else {printf(ab[i])}; print " "c[ab[i]]}}' "$@"
	;;

    detokenize) #[FILE...]; Compresses a list of tokens back into lines of text. Currently ignores linebreaks and creates one continuous stream of tokens.
	awk 'NR!=1 {printf(" ")} {printf($1)}' "$@"
	;;

    kwic) #KEYWORD [FILE...]; Keywords in context. Print all windows of which KEYWORD falls in the center.
	zparseopts -F -D w:=WINDOWSIZE ${c#WINDOWSIZE; Defines the window to be 2*WINDOWSIZE+1 words, centered on KEYWORD. Default is 10.}
	cat "${@:2}" | tr -s '[[:space:]]' '\n' | grep -xC "${WINDOWSIZE[2]:-10}" --group-separator="$GS" "$1" | tr '\n'"$GS" ' \n' | sed -E 's/^ | $//g'
	#handles multiline resets but currently misses keywords in first or last segment of line. #awk -vKEYWORD="$1" -vWINDOW="${WINDOWSIZE[2]:-10}" -v'RS=[[:space:][:cntrl:]]+' 'BEGIN {FULLWINDOW=2*WINDOW+1} $1 {if (KEYWORD==$1) ; ctx[idx++]=$1; idx%=FULLWINDOW; if (ctx[idx+WINDOW%FULLWINDOW]==KEYWORD) for (i=0;i<FULLWINDOW;i++) printf("%s%s",ctx[(idx+i)%FULLWINDOW],i==FULLWINDOW-1?"\n":" ")} RT=="\n" {delete ctx; idx=0}' "${@:2}"
	;;

    mangle) #UNIGRAM [BIGRAM [WINDOW=1]]; Mangles all UNIGRAM if BIGRAM not supplied, otherwise all bigrams formed from UNIGRAM and BIGRAM within WINDOW of each other in a line-separated TOKENS file.
	awk -vU="$1" -vB="$2" -vW="${3:-1}" '{premangle=""; if ($0==U) {if (!B) {premangle=mangle++} else {for (i=0;i<W;i++) {if (words[i]==B) {words[i]=words[i] mangle++; premangle=mangle++}}}} else if ($0==B) for (i=0;i<W;i++) {if (words[i]==U) {words[i]=words[i] mangle++;  premangle=mangle++}}; if (words[0]) print words[0]; for (i=0;i<W-1;i++) words[i]=words[i+1]; words[W-1]=$0 premangle} END {for (i=0;i<W;i++) if (words[i]) print words[i]}'

	#{words[W-1]=$0 mangle++} else words[W-1]=$0
	;;
    
    merge) #FILE...; Merges FILES together with file separator bytes so that they can be streamed.
	awk -vFILESEP="$FS" 'BEGINFILE {print FILESEP FILENAME} {print}' "$@"
	;;
    
    mwe) #MWE...; Replace all multiword expressions MWE in the corpus on stdin with single tokens joined by a separator.
	sed -f <(printf "%s\n" "$@" | sed -E 'h;s/\s+/_/g;x;s/\s+/[[:space:]]\\+/g;G;s/\n/'$US'/;s|^|/^'$FS'/! s'$US'|;s|$|'$US'g|')
	;;
    
    split) #; Splits apart a merged file stream and writes it to separate files.
	awk -vFILESEP="$FS" 'substr($0,1,1)==FILESEP {FNAME=substr($0,2); next} {print > FNAME}'
	;;

    stopwords) #STOPWORDLIST FILE...; Removes stop words in STOPWORDLIST from tokenized FILE...
	grep -vFxf "$@"
	;;
    
    tokens) #[FILE...]; Prints list of whitespace-separated tokens in the corpus.
	tr -s '[[:space:]]' '\n' < "${@:-/dev/stdin}" | grep -v '^$'
	;;

    types) #[FILE...]; Prints list of unique token types in the corpus.
	"$0" tokens | sort -u
	;;
    
    *)
	if [[ -z "$CMD" ]]; then
	    "$0" help
	else
	    "$0" help 1>&2
	    exit 1
	fi
	;;
esac

#!/bin/zsh

#TODO Make mwe match multiline expressions.
#TODO Give merge option to split lines to make large token files readable by other line-based tools. Possibly using null bytes to preserve original linebreaks.

set -o errexit
set -o pipefail
if [[ "${TRACE-0}" == "1" ]]; then set -o xtrace; fi

unset c #Reserved variable for inline comments

FS=$'\034'
GS=$'\035'
US=$'\037'

CMD="$1"
[[ $# -ne 0 ]] && shift
case "$CMD" in
    --help|-h|help) #[SUBCOMMAND]; Prints help text for SUBCOMMAND. If SUBCOMMAND omitted, prints list of subcommands.
	[[ $# -eq 0 ]] && echo 'corpus - A suite of tools for cleaning textual corpora. \nUsage: corpus SUBCOMMAND [ARGUMENTS...]\n\nSubcommands:'
	sed -nE '/\s*'"$1"'\)\s#/'"${1:+,/^\s*;;\s*$/}"'{s/^[[:space:]]*([-|[:alnum:]]+)\)\s#([^;]*); (.*)/'"${${1-\t}:#$1}"'\1 \2\t\3/p; s/.*(\w+)[-+:]*=\w+ \$\{c#(.*); (.*)\}.*/\t-\1 \2\t\3/p}' "$0"
	;;
    
    charset) #[FILE...]; Prints counts of every character used in the corpus, substituting ASCII octal codes for control characters. Also counts filename metadata if corpus merged.
	awk -l ordchr -vRS='(.)' '{c[RT]++} END {asorti(c,ab); for (i in ab) {if (0 <= ord(ab[i]) && ord(ab[i]) <=32) {printf("$'"'"'\\\\%03d'"'"'",ord(ab[i]))} else {printf(ab[i])}; print " "c[ab[i]]}}' "$@"
	;;

    kwic) #KEYWORD [FILE...]; Keywords in context. Print all windows of which KEYWORD falls in the center.
	zparseopts -F -D w:=WINDOWSIZE ${c#WINDOWSIZE; Defines the window to be 2*WINDOWSIZE+1 words, centered on KEYWORD. Default is 10.}
	    cat "${@:2}" | tr -s '[[:space:]]' '\n' | grep -xC "${WINDOWSIZE[2]:-10}" --group-separator="$GS" "$1" | tr '\n'"$GS" ' \n' | sed -E 's/^ | $//g'
	    #handles multiline resets but currently misses keywords in first or last segment of line. #awk -vKEYWORD="$1" -vWINDOW="${WINDOWSIZE[2]:-10}" -v'RS=[[:space:][:cntrl:]]+' 'BEGIN {FULLWINDOW=2*WINDOW+1} $1 {if (KEYWORD==$1) ; ctx[idx++]=$1; idx%=FULLWINDOW; if (ctx[idx+WINDOW%FULLWINDOW]==KEYWORD) for (i=0;i<FULLWINDOW;i++) printf("%s%s",ctx[(idx+i)%FULLWINDOW],i==FULLWINDOW-1?"\n":" ")} RT=="\n" {delete ctx; idx=0}' "${@:2}"
	;;
    
    merge) #FILE...; Merges FILES together with file separator bytes so that they can be streamed.
	awk -vFILESEP="$FS" 'BEGINFILE {print FILESEP FILENAME} {print}' "$@"
	;;
    
    mwe) #MWE...; Replace all multiword expressions MWE in the corpus on stdin with single tokens joined by a separator.
	sed -f <(printf "%s\n" "$@" | sed -E 'h;s/\s+/_/g;x;s/\s+/[[:space:]]\\+/g;G;s/\n/'$US'/;s|^|/^'$FS'/! s'$US'|;s|$|'$US'g|')
	;;
    
    split) #; Splits apart a merged file stream and writes it to separate files.
	awk -vFILESEP="$FS" 'substr($0,1,1)==FILESEP {FNAME=substr($0,2); next} {print > FNAME}'
	;;

    stopwords) #STOPWORDLIST FILE...; Removes stop words in STOPWORDLIST from tokenized FILE...
	grep -vFxf "$@"
	;;
    
    tokens) #[FILE...]; Prints list of whitespace-separated tokens in the corpus.
	tr -s '[[:space:]]' '\n' < "${@:-/dev/stdin}" | grep -v '^$'
	;;

    types) #[FILE...]; Prints list of unique token types in the corpus.
	"$0" tokens | sort -u
	;;
    
    *)
	if [[ -z "$CMD" ]]; then
	    "$0" help
	else
	    "$0" help 1>&2
	    exit 1
	fi
	;;
esac
